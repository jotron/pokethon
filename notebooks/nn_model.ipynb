{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "   - We solve the problem with a neural network. Other approaches like Random Forests were considered but were to hard to train.\n",
    "   - We manually experimented with various hyperparameters, and this is the best performing setup.\n",
    "   - The selection of the actual team happens in a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# For reproducability across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "# Relativ Path to folder with data structure\n",
    "data_path = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission = pd.read_csv(data_path+'01_raw/Submission.csv', delimiter='|')\n",
    "AvailablePokemons = pd.read_csv(data_path+'01_raw/AvailablePokemons.csv', delimiter='|')\n",
    "BattleResults = pd.read_csv(data_path+'01_raw/Battle_Results.csv', delimiter='|')\n",
    "Weakness_Pokemon = pd.read_csv(data_path+'01_raw/Weakness_Pokemon.csv', delimiter='|')\n",
    "AllPokemons = pd.read_csv(data_path+'01_raw/All_Pokemons.csv', delimiter='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pipeline Auxilliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RelativeResult(df):\n",
    "    \"\"\"Return array with Battle Outcomes scaled to [-1, 1]\"\"\"\n",
    "    HP = df.apply(lambda row: row[\"HP_1\"] if row[\"BattleResult\"] > 0 else row[\"HP_2\"], axis=1).values\n",
    "    relativized = df[\"BattleResult\"].values / HP\n",
    "    return relativized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LevelScaler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Return Level scaled to range [0,1]\"\"\"\n",
    "    def __init__(self): \n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        return self  \n",
    "    def transform(self, X):\n",
    "        return X / 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddPokemonTypes(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Add Pokemon Types to the Dataframe, e.g. 'Fire'\"\"\"\n",
    "    def __init__(self): \n",
    "        # Column names. Where Type2_1 is the second type of the first pokemon\n",
    "        self.type_attributes = ['Type1_1', 'Type2_1', 'Type1_2', 'Type2_2']\n",
    "        # Dictionnary for fast access\n",
    "        self.NametoIndex = {sys.intern(name): AllPokemons.loc[AllPokemons[\"Name\"]==name].iloc[0].ID-1 for name in AllPokemons.Name}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self  \n",
    "    \n",
    "    def transform(self, df):\n",
    "        # Copy Dataframe\n",
    "        df = df.copy()\n",
    "        # Directly work on underlying data\n",
    "        values = df.values\n",
    "        # Create array of shape (n, 2) describing pokemon types for convenience\n",
    "        AllPokemonTypes = AllPokemons.loc[:,['Type_1', 'Type_2']]\n",
    "        AllPokemonTypes.Type_2 = AllPokemons.Type_2.fillna(AllPokemons.Type_1)\n",
    "        AllPokemonTypes = AllPokemonTypes.values\n",
    "        \n",
    "        # Old Column indexes\n",
    "        name1_idx = df.columns.get_loc(\"Name_1\")\n",
    "        name2_idx = df.columns.get_loc(\"Name_2\")\n",
    "\n",
    "        # New Columns\n",
    "        Types = np.empty((len(df),4), dtype='U15')\n",
    "\n",
    "        # Go through all rows\n",
    "        for i in range(len(df)):\n",
    "            idx_1 = self.NametoIndex[values[i,name1_idx]]\n",
    "            idx_2 = self.NametoIndex[values[i,name2_idx]]\n",
    "\n",
    "            # Types\n",
    "            Types[i, 0] = AllPokemonTypes[idx_1, 0]\n",
    "            Types[i, 1] = AllPokemonTypes[idx_1, 1]\n",
    "            Types[i, 2] = AllPokemonTypes[idx_2, 0]\n",
    "            Types[i, 3] = AllPokemonTypes[idx_2, 1]\n",
    "\n",
    "        \n",
    "        # Add columns\n",
    "        df[self.type_attributes] = pd.DataFrame(Types, index=df.index)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DuplicateData(df, y):\n",
    "    \"\"\"\n",
    "    Add inverse data to the dataframe for data augmentation. For every row \n",
    "        (pokemon1_stats, pokemon2_stats, result)\n",
    "    add the row\n",
    "        (pokemon2_stats, pokemon1_stats, (-1) * result).\n",
    "    \"\"\"\n",
    "    df_inverse = df.copy()\n",
    "    y_inverse = y.copy()\n",
    "    \n",
    "    # Inverse y\n",
    "    y_inverse = (-1) * y_inverse\n",
    "    \n",
    "    # Inverse df\n",
    "    swap_attributes = ['Name', 'Level', 'HP', 'Attack', 'Defense', 'Sp_Atk', 'Speed', 'Legendary', 'Price']\n",
    "    for attr in swap_attributes:\n",
    "        df_inverse.loc[:, [f'{attr}_1', f'{attr}_2']] = df.loc[:, [f'{attr}_2', f'{attr}_1']].values\n",
    "    df_inverse.loc[:, 'BattleResult'] = - df.loc[:, 'BattleResult'] # just for consistency\n",
    "    \n",
    "    # Merge and Shuffle\n",
    "    df_duplicated = df.append(df_inverse)\n",
    "    y_duplicated = np.concatenate([y, y_inverse], axis=0)\n",
    "    df_duplicated, y_duplicated = sklearn.utils.shuffle(df_duplicated, y_duplicated, random_state=42)\n",
    "    \n",
    "    return df_duplicated, y_duplicated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale labels\n",
    "y = RelativeResult(BattleResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate training data from final test data\n",
    "df_train_val_x, df_test_x, train_val_y, test_y = train_test_split(BattleResults, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data further into validation data.\n",
    "df_train_x, df_val_x, train_y, val_y = train_test_split(df_train_val_x, train_val_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment training data\n",
    "df_train_augm_x, train_augm_y = DuplicateData(df_train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing pipeline and fit sklearn scalers to data\n",
    "numerical_attributes = ['HP_1', 'Attack_1', 'Defense_1', 'Sp_Atk_1', 'Sp_Def_1', 'Speed_1', \n",
    "                          'HP_2', 'Attack_2', 'Defense_2', 'Sp_Atk_2', 'Sp_Def_2', 'Speed_2']\n",
    "type_attributes = ['Type1_1', 'Type2_1', 'Type1_2', 'Type2_2']\n",
    "TypeEncoder = OrdinalEncoder()\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    (\"Add Types\", AddPokemonTypes()),\n",
    "    (\"Individual Feature Preprocessing\", ColumnTransformer([\n",
    "        (\"Drop\", \"drop\", ['Name_1', 'Name_2', 'Price_1', 'Price_2', 'BattleResult']),\n",
    "        (\"Numerical Attributes\", StandardScaler(), numerical_attributes), # Faulty when not using augmented data\n",
    "        (\"Boolean\", \"passthrough\", ['Legendary_1', 'Legendary_2']),\n",
    "        (\"Level\" , LevelScaler(), ['Level_1', 'Level_2']),\n",
    "        (\"Weather\", OrdinalEncoder(), ['WeatherAndTime']),\n",
    "        (\"Types\", TypeEncoder, type_attributes)\n",
    "    ])),\n",
    "])\n",
    "full_pipeline.fit(df_train_augm_x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformation\n",
    "train_augm_x = full_pipeline.transform(df_train_augm_x)\n",
    "val_x = full_pipeline.transform(df_val_x)\n",
    "test_x = full_pipeline.transform(df_test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional Model Architecture\n",
    "# Categorical Data is embedded. Same embedding is used for Pokemon First Nature and Second Nature (Type).\n",
    "\"\"\" Input columns are as follows:\n",
    "- input[0:16] are numerical attributes\n",
    "- input[16] is Ordinal WeatherAndTime\n",
    "- input[17] is Ordinal Pokemon_1 First Nature\n",
    "- input[18] is Ordinal Pokemon_1 Second Nature\n",
    "- input[19] is Ordinal Pokemon_2 First Nature\n",
    "- input[20] is Ordinal Pokemon_2 Second Nature\n",
    "\"\"\"\n",
    "model_input = layers.Input(shape=(21,), name='ModelInput')\n",
    "\n",
    "# Embeddings\n",
    "type_emb = layers.Embedding(input_dim=18, output_dim=8, name=\"Pokemon1Type\")\n",
    "type1_emb_1 = type_emb(model_input[:,17])\n",
    "type2_emb_1 = type_emb(model_input[:,18])\n",
    "type1_emb_2 = type_emb(model_input[:,19])\n",
    "type2_emb_2 = type_emb(model_input[:,20])\n",
    "weather_emb = layers.Embedding(input_dim=5, output_dim=3, name=\"WeatherEmbedding\")(model_input[:, 16])\n",
    "\n",
    "# Linear Network\n",
    "x = layers.Concatenate()([model_input[:, 0:16], weather_emb, type1_emb_1, type2_emb_1, type1_emb_2, type2_emb_2])\n",
    "x = layers.Dense(200, activation='relu')(x)\n",
    "x = layers.Dense(200, activation='relu')(x)\n",
    "x = layers.Dense(200, activation='relu')(x)\n",
    "model_output = layers.Dense(1, activation='tanh')(x)\n",
    "\n",
    "model = keras.Model(inputs=model_input, outputs=model_output)\n",
    "\n",
    "# plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(learning_rate, batch_size, epochs):\n",
    "    \"\"\"Train model with specific learning rate and batch_size\"\"\"\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
    "                  metrics=['RootMeanSquaredError'])\n",
    "    \n",
    "    history = model.fit(train_augm_x, train_augm_y, epochs=epochs,\n",
    "                        validation_data=(val_x, val_y),\n",
    "                        batch_size=batch_size)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "3824/3824 [==============================] - 20s 5ms/step - loss: 0.0238 - root_mean_squared_error: 0.1544 - val_loss: 0.0113 - val_root_mean_squared_error: 0.1061\n",
      "Epoch 2/12\n",
      "3824/3824 [==============================] - 20s 5ms/step - loss: 0.0106 - root_mean_squared_error: 0.1028 - val_loss: 0.0101 - val_root_mean_squared_error: 0.1004\n",
      "Epoch 3/12\n",
      "3824/3824 [==============================] - 20s 5ms/step - loss: 0.0086 - root_mean_squared_error: 0.0928 - val_loss: 0.0117 - val_root_mean_squared_error: 0.1081\n",
      "Epoch 4/12\n",
      "3824/3824 [==============================] - 20s 5ms/step - loss: 0.0076 - root_mean_squared_error: 0.0874 - val_loss: 0.0089 - val_root_mean_squared_error: 0.0945\n",
      "Epoch 5/12\n",
      "3824/3824 [==============================] - 20s 5ms/step - loss: 0.0071 - root_mean_squared_error: 0.0843 - val_loss: 0.0063 - val_root_mean_squared_error: 0.0796\n",
      "Epoch 6/12\n",
      "3824/3824 [==============================] - 20s 5ms/step - loss: 0.0068 - root_mean_squared_error: 0.0823 - val_loss: 0.0070 - val_root_mean_squared_error: 0.0838\n",
      "Epoch 7/12\n",
      "3824/3824 [==============================] - 20s 5ms/step - loss: 0.0065 - root_mean_squared_error: 0.0808 - val_loss: 0.0062 - val_root_mean_squared_error: 0.0789\n",
      "Epoch 8/12\n",
      "3824/3824 [==============================] - 20s 5ms/step - loss: 0.0064 - root_mean_squared_error: 0.0799 - val_loss: 0.0055 - val_root_mean_squared_error: 0.0742\n",
      "Epoch 9/12\n",
      "3824/3824 [==============================] - 20s 5ms/step - loss: 0.0063 - root_mean_squared_error: 0.0793 - val_loss: 0.0065 - val_root_mean_squared_error: 0.0804\n",
      "Epoch 10/12\n",
      "3824/3824 [==============================] - 20s 5ms/step - loss: 0.0062 - root_mean_squared_error: 0.0787 - val_loss: 0.0058 - val_root_mean_squared_error: 0.0765\n",
      "Epoch 11/12\n",
      "3824/3824 [==============================] - 20s 5ms/step - loss: 0.0062 - root_mean_squared_error: 0.0784 - val_loss: 0.0066 - val_root_mean_squared_error: 0.0810\n",
      "Epoch 12/12\n",
      "3824/3824 [==============================] - 20s 5ms/step - loss: 0.0061 - root_mean_squared_error: 0.0780 - val_loss: 0.0054 - val_root_mean_squared_error: 0.0737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad7c1725c0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(3e-3, 1024, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "61182/61182 [==============================] - 135s 2ms/step - loss: 0.0042 - root_mean_squared_error: 0.0648 - val_loss: 0.0043 - val_root_mean_squared_error: 0.0653\n",
      "Epoch 2/2\n",
      "61182/61182 [==============================] - 135s 2ms/step - loss: 0.0040 - root_mean_squared_error: 0.0629 - val_loss: 0.0042 - val_root_mean_squared_error: 0.0646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fad7c274780>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(1e-5, 64, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load/Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savemodel = True\n",
    "loadmodel = False\n",
    "if (savemodel): model.save(data_path+'02_models/nn_model', overwrite=False)\n",
    "if (loadmodel): model = keras.models.load_model(data_path+'02_models/nn_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inference_data():\n",
    "    \"\"\"\n",
    "    Generate Inference data to predict all battles between available pokemons and adversary pokemons.\n",
    "    Return the same Format as BattleResults with additional PokemonID column.\n",
    "    \"\"\"\n",
    "    # Repeat every row 6 times (there are 6 enemy pokemons)\n",
    "    merged = pd.DataFrame(np.repeat(AvailablePokemons.values,6,axis=0), columns=AvailablePokemons.columns)\n",
    "    # Drop empty PokemonID from Submission\n",
    "    Submission_dropped = Submission.drop(columns=['SelectedPokemonID'])\n",
    "    # Concat with Submission\n",
    "    merged = merged.apply(lambda row: pd.concat([row, Submission_dropped.iloc[row.name%6]]), axis=1)\n",
    "    return merged\n",
    "\n",
    "def export(predictions):\n",
    "    \"\"\"\n",
    "    Generate comprehensive table describing the outcome of the inference battles.\n",
    "    \"\"\"\n",
    "    predictions = predictions.reshape(-1)\n",
    "    table = np.empty((len(AvailablePokemons), 7))\n",
    "    for i in range(len(AvailablePokemons)):\n",
    "        table[i, 0] = i+1\n",
    "        table[i, 1:] = predictions[i*6: (i+1)*6]\n",
    "    df = pd.DataFrame(data=table, columns=['SelectedPokemonID']+list(Submission.Name_2.values))\n",
    "    return df.astype({'SelectedPokemonID': int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Inference Data\n",
    "inference_data = create_inference_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joel/.local/share/virtualenvs/pokethon-gj-dbL-j/lib/python3.6/site-packages/sklearn/compose/_column_transformer.py:440: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Transform Data, Predict outcomes, Export to csv\n",
    "save_inference = True\n",
    "\n",
    "x_inference = full_pipeline.transform(inference_data)\n",
    "inference_predictions = model.predict(x_inference)\n",
    "output = export(inference_predictions)\n",
    "\n",
    "if (save_inference): output.to_csv(path_or_buf=data_path+'03_model_output/nn_model/inference.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8498/8498 [==============================] - 7s 811us/step - loss: 0.0042 - root_mean_squared_error: 0.0645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0041633062064647675, 0.06452368944883347]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_x, test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PipenvPokethon",
   "language": "python",
   "name": "pipenvpokethon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
